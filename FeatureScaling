{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D","metadata":{"ExecuteTime":{"end_time":"2024-05-31T02:19:56.515452Z","start_time":"2024-05-31T02:19:56.512772Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 2D array\n# Number of data points\nn = 1000\nweights = [5, 0.1]\nbias = 2\n# Generate random data for x, y, and z\nx = np.random.rand(n)\ny = np.random.rand(n) * 10\nz = weights[0]*x + weights[1]*y + bias + np.random.rand(n) * 0.1  # Add some noise\n\n# Stack the arrays\ndata = np.column_stack((x, y, z))\n# Separate features and target variable\nX = data[:, :2]  # x and y coordinates\ny = data[:, 2]  # z coordinate","metadata":{"ExecuteTime":{"end_time":"2024-05-31T02:19:56.522042Z","start_time":"2024-05-31T02:19:56.518492Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_scaling(X):\n    for i in range(X.shape[1]):\n        X[:, i] = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])","metadata":{"ExecuteTime":{"end_time":"2024-05-31T02:19:56.525752Z","start_time":"2024-05-31T02:19:56.523524Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = np.zeros(X.shape[1])\nb = 0\nlearning_rate = 0.9\nmax_epochs = 1000\nfeature_scaling(X)","metadata":{"ExecuteTime":{"end_time":"2024-05-31T02:19:56.529274Z","start_time":"2024-05-31T02:19:56.526958Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef predict(X, w, b):\n    return np.dot(X, w) + b\n\n\ndef cost(X, w, b, y):\n    return np.mean((predict(X, w, b) - y) ** 2)\n\n\ndef gradient(X, w, b, y):\n    y_pred = predict(X, w, b)\n    dw = np.mean((y_pred - y).reshape(X.shape[0], 1) * X, axis=0)\n    db = np.mean(y_pred - y)\n    return dw, db\n\n\ndef train(X, y, w, b, learning_rate, epochs):\n    costs = []\n    prev_cost = 0\n    for i in range(epochs):\n        dw, db = gradient(X, w, b, y)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        cost_value = cost(X, w, b, y)\n        costs.append(cost_value)\n        if i % 100 == 0:\n            print(f'Epoch {i}, Cost: {cost_value}')\n        \n        # Check if the change in cost is less than 10^-3\n        if abs(prev_cost - cost_value) < 10**-5:\n            print(f'Training stopped at epoch {i}')\n            break\n        \n        prev_cost = cost_value\n\n    return w, b, costs\n# Train the model and get the costs\nw, b, costs = train(X_train, y_train, w, b, learning_rate, max_epochs)\n","metadata":{"ExecuteTime":{"end_time":"2024-05-31T02:19:56.536450Z","start_time":"2024-05-31T02:19:56.530783Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the costs\nplt.plot(costs)\nplt.xlabel('Epoch')\nplt.ylabel('Cost')\nplt.title('Cost over training cycles')\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2024-05-31T02:19:56.604972Z","start_time":"2024-05-31T02:19:56.537363Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the values on the test data\ny_pred = predict(X_test, w, b)\n\n# Calculate the cost\ncost_value = cost(X_test, w, b, y_test)\n\n# Print the cost\nprint(f\"The cost of the model is: {cost_value}\")","metadata":{"ExecuteTime":{"end_time":"2024-05-31T02:19:56.608517Z","start_time":"2024-05-31T02:19:56.605691Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nIn our experiment, we observed the impact of feature scaling and learning rate optimization on the training process of our model.\n\n**Without Feature Scaling and Learning Rate Optimization:**\n- The model took 650 training epochs to converge. This indicates that the model was struggling to find the optimal point in the high-dimensional space, which resulted in a longer training time.\n\n**With Feature Scaling and Learning Rate Optimization:**\n- The model took only 4 training epochs to converge. This is a significant improvement over the previous scenario. By scaling the features to a similar range, we helped the model to converge faster. Additionally, optimizing the learning rate allowed the model to make more efficient steps towards the optimal point.\n\nThis experiment clearly shows the importance of feature scaling and learning rate optimization in the training process of a machine learning model. By applying these techniques, we can significantly improve the efficiency of our model training.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}